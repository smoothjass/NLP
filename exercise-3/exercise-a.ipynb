{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:30.681675500Z",
     "start_time": "2023-10-13T07:48:30.365086800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = '''When I was a little girl I used to read fairy tales. In fairy tales you meet Prince Charming and he's everything you ever wanted. In fairy tales the bad guy is very easy to spot. The bad guy is always wearing a black cape so you always know who he is. Then you grow up and you realize that Prince Charming is not as easy to find as you thought. You realize the bad guy is not wearing a black cape and he's not easy to spot; he's really funny, and he makes you laugh, and he has perfect hair.\n",
    "I suffer from girl-next-door-itis where the guy is friends with you and that's it.\n",
    "I’ve learned that you can’t predict [love] or plan for it. For someone like me who is obsessed with organization and planning, I love the idea that love is the one exception to that. Love is the one wild card.\n",
    "In high school, I used to think it was like sooooo cool if a guy had an awesome car. Now none of that matters. These days I look for character, and honesty, and trust.\n",
    "I write songs about my adventures and misadventures, most of which concern love. Love is a tricky business. But if it wasn't, I wouldn't be so enthralled with it. Lately I've come to a wonderful realization that makes me even more fascinated by it: I have no idea what I'm doing when it comes to love. No one does! There's no pattern to it, except that it happens to all of us, of course. I can't plan for it. I can't predict how it'll end up. Because love is unpredictable and it's frustrating and it's tragic and it's beautiful. And even though there's no way to feel like I'm an expert at it, it's worth writing songs about -- more than anything else I've ever experienced in my life.\n",
    "I love you like I love sparkles and having the last word. And that's real love.\n",
    "I write a lot of songs about love and I think that’s because to me love seems like this huge complicated thing. But it seems like every once in a while, two people get it figured out, two people get it right. And so I think the rest of us, we walk around daydreaming about what that might be like. To find that one great love, where all of a sudden everything that seemed to be so complicated, became simple. And everything that used to seem so wrong all of a sudden seemed right because you were with the person who made you feel fearless.\n",
    "Why leave when you can stay, why stay if you have to go\n",
    "If you're horrible to me, I'm going to write a song about it, and you won't like it. That's how I operate.\n",
    "Just because as human beings, what we can't have is what we reply in our head over and over again before we go to sleep.\n",
    "I still love sparkles and grocery shopping and really old cats that are only nice to you half the time. I still love writing in my journal and wearing dresses all the time and staring at chandeliers.\n",
    "I love these people so much and getting to go out there to look them in the eye and tell them that is my favorite part of the night.\n",
    "We should love, not fall in love because everything that falls breaks.\n",
    "In a relationship each person should support the other; they should lift each other up.\n",
    "I'm not opposed to falling in love\n",
    "You need to be happy with yourself or you'll never be able to be happy in a relationship.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.1: Preprocessing\n",
    "1. Lowercase the text.\n",
    "2. Remove digits from the text.\n",
    "3. Remove punctuation marks.\n",
    "4. Remove superfluous blanks.\n",
    "Hints:\n",
    "• The Python package ’re’, for regular expressions, provides the ’sub’ function to remove substrings with\n",
    "certain patterns.\n",
    "• The Python package ’string’ contains a collection of string constants, such as punctuation marks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d8a95d954b70fa"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:31.283255300Z",
     "start_time": "2023-10-13T07:48:30.418507700Z"
    }
   },
   "id": "69a54d3334ec4f2c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I was a little girl I used to read fairy tales. In fairy tales you meet Prince Charming and he's everything you ever wanted. In fairy tales the bad guy is very easy to spot. The bad guy is always wearing a black cape so you always know who he is. Then you grow up and you realize that Prince Charming is not as easy to find as you thought. You realize the bad guy is not wearing a black cape and he's not easy to spot; he's really funny, and he makes you laugh, and he has perfect hair.\n",
      "I suffer from girl-next-door-itis where the guy is friends with you and that's it.\n",
      "I’ve learned that you can’t predict [love] or plan for it. For someone like me who is obsessed with organization and planning, I love the idea that love is the one exception to that. Love is the one wild card.\n",
      "In high school, I used to think it was like sooooo cool if a guy had an awesome car. Now none of that matters. These days I look for character, and honesty, and trust.\n",
      "I write songs about my adventures and misadventures, most of which concern love. Love is a tricky business. But if it wasn't, I wouldn't be so enthralled with it. Lately I've come to a wonderful realization that makes me even more fascinated by it: I have no idea what I'm doing when it comes to love. No one does! There's no pattern to it, except that it happens to all of us, of course. I can't plan for it. I can't predict how it'll end up. Because love is unpredictable and it's frustrating and it's tragic and it's beautiful. And even though there's no way to feel like I'm an expert at it, it's worth writing songs about -- more than anything else I've ever experienced in my life.\n",
      "I love you like I love sparkles and having the last word. And that's real love.\n",
      "I write a lot of songs about love and I think that’s because to me love seems like this huge complicated thing. But it seems like every once in a while, two people get it figured out, two people get it right. And so I think the rest of us, we walk around daydreaming about what that might be like. To find that one great love, where all of a sudden everything that seemed to be so complicated, became simple. And everything that used to seem so wrong all of a sudden seemed right because you were with the person who made you feel fearless.\n",
      "Why leave when you can stay, why stay if you have to go\n",
      "If you're horrible to me, I'm going to write a song about it, and you won't like it. That's how I operate.\n",
      "Just because as human beings, what we can't have is what we reply in our head over and over again before we go to sleep.\n",
      "I still love sparkles and grocery shopping and really old cats that are only nice to you half the time. I still love writing in my journal and wearing dresses all the time and staring at chandeliers.\n",
      "I love these people so much and getting to go out there to look them in the eye and tell them that is my favorite part of the night.\n",
      "We should love, not fall in love because everything that falls breaks.\n",
      "In a relationship each person should support the other; they should lift each other up.\n",
      "I'm not opposed to falling in love\n",
      "You need to be happy with yourself or you'll never be able to be happy in a relationship.\n"
     ]
    }
   ],
   "source": [
    "# explore the data\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:31.499482100Z",
     "start_time": "2023-10-13T07:48:30.425446700Z"
    }
   },
   "id": "fb8ebff8ccbdea6d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was a little girl i used to read fairy tales. in fairy tales you meet prince charming and he's everything you ever wanted. in fairy tales the bad guy is very easy to spot. the bad guy is always wearing a black cape so you always know who he is. then you grow up and you realize that prince charming is not as easy to find as you thought. you realize the bad guy is not wearing a black cape and he's not easy to spot; he's really funny, and he makes you laugh, and he has perfect hair.\n",
      "i suffer from girl-next-door-itis where the guy is friends with you and that's it.\n",
      "i’ve learned that you can’t predict [love] or plan for it. for someone like me who is obsessed with organization and planning, i love the idea that love is the one exception to that. love is the one wild card.\n",
      "in high school, i used to think it was like sooooo cool if a guy had an awesome car. now none of that matters. these days i look for character, and honesty, and trust.\n",
      "i write songs about my adventures and misadventures, most of which concern love. love is a tricky business. but if it wasn't, i wouldn't be so enthralled with it. lately i've come to a wonderful realization that makes me even more fascinated by it: i have no idea what i'm doing when it comes to love. no one does! there's no pattern to it, except that it happens to all of us, of course. i can't plan for it. i can't predict how it'll end up. because love is unpredictable and it's frustrating and it's tragic and it's beautiful. and even though there's no way to feel like i'm an expert at it, it's worth writing songs about -- more than anything else i've ever experienced in my life.\n",
      "i love you like i love sparkles and having the last word. and that's real love.\n",
      "i write a lot of songs about love and i think that’s because to me love seems like this huge complicated thing. but it seems like every once in a while, two people get it figured out, two people get it right. and so i think the rest of us, we walk around daydreaming about what that might be like. to find that one great love, where all of a sudden everything that seemed to be so complicated, became simple. and everything that used to seem so wrong all of a sudden seemed right because you were with the person who made you feel fearless.\n",
      "why leave when you can stay, why stay if you have to go\n",
      "if you're horrible to me, i'm going to write a song about it, and you won't like it. that's how i operate.\n",
      "just because as human beings, what we can't have is what we reply in our head over and over again before we go to sleep.\n",
      "i still love sparkles and grocery shopping and really old cats that are only nice to you half the time. i still love writing in my journal and wearing dresses all the time and staring at chandeliers.\n",
      "i love these people so much and getting to go out there to look them in the eye and tell them that is my favorite part of the night.\n",
      "we should love, not fall in love because everything that falls breaks.\n",
      "in a relationship each person should support the other; they should lift each other up.\n",
      "i'm not opposed to falling in love\n",
      "you need to be happy with yourself or you'll never be able to be happy in a relationship.\n"
     ]
    }
   ],
   "source": [
    "# lowercase and explore\n",
    "data_lower_case = data.lower()\n",
    "print(data_lower_case)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:31.552894600Z",
     "start_time": "2023-10-13T07:48:30.441082Z"
    }
   },
   "id": "f97fcb0bcb059b76"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was a little girl i used to read fairy tales. in fairy tales you meet prince charming and he's everything you ever wanted. in fairy tales the bad guy is very easy to spot. the bad guy is always wearing a black cape so you always know who he is. then you grow up and you realize that prince charming is not as easy to find as you thought. you realize the bad guy is not wearing a black cape and he's not easy to spot; he's really funny, and he makes you laugh, and he has perfect hair.\n",
      "i suffer from girl-next-door-itis where the guy is friends with you and that's it.\n",
      "i’ve learned that you can’t predict [love] or plan for it. for someone like me who is obsessed with organization and planning, i love the idea that love is the one exception to that. love is the one wild card.\n",
      "in high school, i used to think it was like sooooo cool if a guy had an awesome car. now none of that matters. these days i look for character, and honesty, and trust.\n",
      "i write songs about my adventures and misadventures, most of which concern love. love is a tricky business. but if it wasn't, i wouldn't be so enthralled with it. lately i've come to a wonderful realization that makes me even more fascinated by it: i have no idea what i'm doing when it comes to love. no one does! there's no pattern to it, except that it happens to all of us, of course. i can't plan for it. i can't predict how it'll end up. because love is unpredictable and it's frustrating and it's tragic and it's beautiful. and even though there's no way to feel like i'm an expert at it, it's worth writing songs about -- more than anything else i've ever experienced in my life.\n",
      "i love you like i love sparkles and having the last word. and that's real love.\n",
      "i write a lot of songs about love and i think that’s because to me love seems like this huge complicated thing. but it seems like every once in a while, two people get it figured out, two people get it right. and so i think the rest of us, we walk around daydreaming about what that might be like. to find that one great love, where all of a sudden everything that seemed to be so complicated, became simple. and everything that used to seem so wrong all of a sudden seemed right because you were with the person who made you feel fearless.\n",
      "why leave when you can stay, why stay if you have to go\n",
      "if you're horrible to me, i'm going to write a song about it, and you won't like it. that's how i operate.\n",
      "just because as human beings, what we can't have is what we reply in our head over and over again before we go to sleep.\n",
      "i still love sparkles and grocery shopping and really old cats that are only nice to you half the time. i still love writing in my journal and wearing dresses all the time and staring at chandeliers.\n",
      "i love these people so much and getting to go out there to look them in the eye and tell them that is my favorite part of the night.\n",
      "we should love, not fall in love because everything that falls breaks.\n",
      "in a relationship each person should support the other; they should lift each other up.\n",
      "i'm not opposed to falling in love\n",
      "you need to be happy with yourself or you'll never be able to be happy in a relationship.\n"
     ]
    }
   ],
   "source": [
    "# remove digits and explore\n",
    "data_digits_removed = re.sub('\\d+', '', data_lower_case)\n",
    "print(data_digits_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:31.721835500Z",
     "start_time": "2023-10-13T07:48:30.456701300Z"
    }
   },
   "id": "bc9789842b60fc13"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# not sure if the data even had digits. let's see\n",
    "print(re.search('\\d+', data))\n",
    "# no digits in the text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:31.853390200Z",
     "start_time": "2023-10-13T07:48:30.472325400Z"
    }
   },
   "id": "9a1e026ca119841d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was a little girl i used to read fairy tales in fairy tales you meet prince charming and hes everything you ever wanted in fairy tales the bad guy is very easy to spot the bad guy is always wearing a black cape so you always know who he is then you grow up and you realize that prince charming is not as easy to find as you thought you realize the bad guy is not wearing a black cape and hes not easy to spot hes really funny and he makes you laugh and he has perfect hair\n",
      "i suffer from girlnextdooritis where the guy is friends with you and thats it\n",
      "i’ve learned that you can’t predict love or plan for it for someone like me who is obsessed with organization and planning i love the idea that love is the one exception to that love is the one wild card\n",
      "in high school i used to think it was like sooooo cool if a guy had an awesome car now none of that matters these days i look for character and honesty and trust\n",
      "i write songs about my adventures and misadventures most of which concern love love is a tricky business but if it wasnt i wouldnt be so enthralled with it lately ive come to a wonderful realization that makes me even more fascinated by it i have no idea what im doing when it comes to love no one does theres no pattern to it except that it happens to all of us of course i cant plan for it i cant predict how itll end up because love is unpredictable and its frustrating and its tragic and its beautiful and even though theres no way to feel like im an expert at it its worth writing songs about  more than anything else ive ever experienced in my life\n",
      "i love you like i love sparkles and having the last word and thats real love\n",
      "i write a lot of songs about love and i think that’s because to me love seems like this huge complicated thing but it seems like every once in a while two people get it figured out two people get it right and so i think the rest of us we walk around daydreaming about what that might be like to find that one great love where all of a sudden everything that seemed to be so complicated became simple and everything that used to seem so wrong all of a sudden seemed right because you were with the person who made you feel fearless\n",
      "why leave when you can stay why stay if you have to go\n",
      "if youre horrible to me im going to write a song about it and you wont like it thats how i operate\n",
      "just because as human beings what we cant have is what we reply in our head over and over again before we go to sleep\n",
      "i still love sparkles and grocery shopping and really old cats that are only nice to you half the time i still love writing in my journal and wearing dresses all the time and staring at chandeliers\n",
      "i love these people so much and getting to go out there to look them in the eye and tell them that is my favorite part of the night\n",
      "we should love not fall in love because everything that falls breaks\n",
      "in a relationship each person should support the other they should lift each other up\n",
      "im not opposed to falling in love\n",
      "you need to be happy with yourself or youll never be able to be happy in a relationship\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation and look at the data again\n",
    "data_punctuation_removed = re.sub('[%s]' % re.escape(string.punctuation), '', data_digits_removed)\n",
    "print(data_punctuation_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:31.953698800Z",
     "start_time": "2023-10-13T07:48:30.481631200Z"
    }
   },
   "id": "bcd6de39d01a160e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was a little girl i used to read fairy tales in fairy tales you meet prince charming and hes everything you ever wanted in fairy tales the bad guy is very easy to spot the bad guy is always wearing a black cape so you always know who he is then you grow up and you realize that prince charming is not as easy to find as you thought you realize the bad guy is not wearing a black cape and hes not easy to spot hes really funny and he makes you laugh and he has perfect hair\n",
      "i suffer from girlnextdooritis where the guy is friends with you and thats it\n",
      "i’ve learned that you can’t predict love or plan for it for someone like me who is obsessed with organization and planning i love the idea that love is the one exception to that love is the one wild card\n",
      "in high school i used to think it was like sooooo cool if a guy had an awesome car now none of that matters these days i look for character and honesty and trust\n",
      "i write songs about my adventures and misadventures most of which concern love love is a tricky business but if it wasnt i wouldnt be so enthralled with it lately ive come to a wonderful realization that makes me even more fascinated by it i have no idea what im doing when it comes to love no one does theres no pattern to it except that it happens to all of us of course i cant plan for it i cant predict how itll end up because love is unpredictable and its frustrating and its tragic and its beautiful and even though theres no way to feel like im an expert at it its worth writing songs about  more than anything else ive ever experienced in my life\n",
      "i love you like i love sparkles and having the last word and thats real love\n",
      "i write a lot of songs about love and i think that’s because to me love seems like this huge complicated thing but it seems like every once in a while two people get it figured out two people get it right and so i think the rest of us we walk around daydreaming about what that might be like to find that one great love where all of a sudden everything that seemed to be so complicated became simple and everything that used to seem so wrong all of a sudden seemed right because you were with the person who made you feel fearless\n",
      "why leave when you can stay why stay if you have to go\n",
      "if youre horrible to me im going to write a song about it and you wont like it thats how i operate\n",
      "just because as human beings what we cant have is what we reply in our head over and over again before we go to sleep\n",
      "i still love sparkles and grocery shopping and really old cats that are only nice to you half the time i still love writing in my journal and wearing dresses all the time and staring at chandeliers\n",
      "i love these people so much and getting to go out there to look them in the eye and tell them that is my favorite part of the night\n",
      "we should love not fall in love because everything that falls breaks\n",
      "in a relationship each person should support the other they should lift each other up\n",
      "im not opposed to falling in love\n",
      "you need to be happy with yourself or youll never be able to be happy in a relationship\n"
     ]
    }
   ],
   "source": [
    "# remove extra blanks\n",
    "data_superfluous_blanks_removed= re.sub(r'\\t{2,}', ' ', data_punctuation_removed)\n",
    "print(data_superfluous_blanks_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:32.138704200Z",
     "start_time": "2023-10-13T07:48:30.497260100Z"
    }
   },
   "id": "8ff5cf8ac017bceb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# again, i don't think there were any extra blanks. let's see\n",
    "print(re.search(r'\\t{2,}', data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:32.201231700Z",
     "start_time": "2023-10-13T07:48:30.518864100Z"
    }
   },
   "id": "2cb1d73e4188e3b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.2: Tokenization\n",
    "1. Tokenize the text.\n",
    "2. Remove stopwords.\n",
    "Hints:\n",
    "• The Python package ’nltk.tokenize’ includes the function ’tokenize’.\n",
    "• The Python package ’spacy’ includes collections of stopwords for many languages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2f4bd58f61b1d8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jassi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:37.176766100Z",
     "start_time": "2023-10-13T07:48:30.534511500Z"
    }
   },
   "id": "53aedb58bfb617f5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'i', 'was', 'a', 'little', 'girl', 'i', 'used', 'to', 'read', 'fairy', 'tales', 'in', 'fairy', 'tales', 'you', 'meet', 'prince', 'charming', 'and', 'hes', 'everything', 'you', 'ever', 'wanted', 'in', 'fairy', 'tales', 'the', 'bad', 'guy', 'is', 'very', 'easy', 'to', 'spot', 'the', 'bad', 'guy', 'is', 'always', 'wearing', 'a', 'black', 'cape', 'so', 'you', 'always', 'know', 'who', 'he', 'is', 'then', 'you', 'grow', 'up', 'and', 'you', 'realize', 'that', 'prince', 'charming', 'is', 'not', 'as', 'easy', 'to', 'find', 'as', 'you', 'thought', 'you', 'realize', 'the', 'bad', 'guy', 'is', 'not', 'wearing', 'a', 'black', 'cape', 'and', 'hes', 'not', 'easy', 'to', 'spot', 'hes', 'really', 'funny', 'and', 'he', 'makes', 'you', 'laugh', 'and', 'he', 'has', 'perfect', 'hair', 'i', 'suffer', 'from', 'girlnextdooritis', 'where', 'the', 'guy', 'is', 'friends', 'with', 'you', 'and', 'thats', 'it', 'i', '’', 've', 'learned', 'that', 'you', 'can', '’', 't', 'predict', 'love', 'or', 'plan', 'for', 'it', 'for', 'someone', 'like', 'me', 'who', 'is', 'obsessed', 'with', 'organization', 'and', 'planning', 'i', 'love', 'the', 'idea', 'that', 'love', 'is', 'the', 'one', 'exception', 'to', 'that', 'love', 'is', 'the', 'one', 'wild', 'card', 'in', 'high', 'school', 'i', 'used', 'to', 'think', 'it', 'was', 'like', 'sooooo', 'cool', 'if', 'a', 'guy', 'had', 'an', 'awesome', 'car', 'now', 'none', 'of', 'that', 'matters', 'these', 'days', 'i', 'look', 'for', 'character', 'and', 'honesty', 'and', 'trust', 'i', 'write', 'songs', 'about', 'my', 'adventures', 'and', 'misadventures', 'most', 'of', 'which', 'concern', 'love', 'love', 'is', 'a', 'tricky', 'business', 'but', 'if', 'it', 'wasnt', 'i', 'wouldnt', 'be', 'so', 'enthralled', 'with', 'it', 'lately', 'ive', 'come', 'to', 'a', 'wonderful', 'realization', 'that', 'makes', 'me', 'even', 'more', 'fascinated', 'by', 'it', 'i', 'have', 'no', 'idea', 'what', 'im', 'doing', 'when', 'it', 'comes', 'to', 'love', 'no', 'one', 'does', 'theres', 'no', 'pattern', 'to', 'it', 'except', 'that', 'it', 'happens', 'to', 'all', 'of', 'us', 'of', 'course', 'i', 'cant', 'plan', 'for', 'it', 'i', 'cant', 'predict', 'how', 'itll', 'end', 'up', 'because', 'love', 'is', 'unpredictable', 'and', 'its', 'frustrating', 'and', 'its', 'tragic', 'and', 'its', 'beautiful', 'and', 'even', 'though', 'theres', 'no', 'way', 'to', 'feel', 'like', 'im', 'an', 'expert', 'at', 'it', 'its', 'worth', 'writing', 'songs', 'about', 'more', 'than', 'anything', 'else', 'ive', 'ever', 'experienced', 'in', 'my', 'life', 'i', 'love', 'you', 'like', 'i', 'love', 'sparkles', 'and', 'having', 'the', 'last', 'word', 'and', 'thats', 'real', 'love', 'i', 'write', 'a', 'lot', 'of', 'songs', 'about', 'love', 'and', 'i', 'think', 'that', '’', 's', 'because', 'to', 'me', 'love', 'seems', 'like', 'this', 'huge', 'complicated', 'thing', 'but', 'it', 'seems', 'like', 'every', 'once', 'in', 'a', 'while', 'two', 'people', 'get', 'it', 'figured', 'out', 'two', 'people', 'get', 'it', 'right', 'and', 'so', 'i', 'think', 'the', 'rest', 'of', 'us', 'we', 'walk', 'around', 'daydreaming', 'about', 'what', 'that', 'might', 'be', 'like', 'to', 'find', 'that', 'one', 'great', 'love', 'where', 'all', 'of', 'a', 'sudden', 'everything', 'that', 'seemed', 'to', 'be', 'so', 'complicated', 'became', 'simple', 'and', 'everything', 'that', 'used', 'to', 'seem', 'so', 'wrong', 'all', 'of', 'a', 'sudden', 'seemed', 'right', 'because', 'you', 'were', 'with', 'the', 'person', 'who', 'made', 'you', 'feel', 'fearless', 'why', 'leave', 'when', 'you', 'can', 'stay', 'why', 'stay', 'if', 'you', 'have', 'to', 'go', 'if', 'youre', 'horrible', 'to', 'me', 'im', 'going', 'to', 'write', 'a', 'song', 'about', 'it', 'and', 'you', 'wont', 'like', 'it', 'thats', 'how', 'i', 'operate', 'just', 'because', 'as', 'human', 'beings', 'what', 'we', 'cant', 'have', 'is', 'what', 'we', 'reply', 'in', 'our', 'head', 'over', 'and', 'over', 'again', 'before', 'we', 'go', 'to', 'sleep', 'i', 'still', 'love', 'sparkles', 'and', 'grocery', 'shopping', 'and', 'really', 'old', 'cats', 'that', 'are', 'only', 'nice', 'to', 'you', 'half', 'the', 'time', 'i', 'still', 'love', 'writing', 'in', 'my', 'journal', 'and', 'wearing', 'dresses', 'all', 'the', 'time', 'and', 'staring', 'at', 'chandeliers', 'i', 'love', 'these', 'people', 'so', 'much', 'and', 'getting', 'to', 'go', 'out', 'there', 'to', 'look', 'them', 'in', 'the', 'eye', 'and', 'tell', 'them', 'that', 'is', 'my', 'favorite', 'part', 'of', 'the', 'night', 'we', 'should', 'love', 'not', 'fall', 'in', 'love', 'because', 'everything', 'that', 'falls', 'breaks', 'in', 'a', 'relationship', 'each', 'person', 'should', 'support', 'the', 'other', 'they', 'should', 'lift', 'each', 'other', 'up', 'im', 'not', 'opposed', 'to', 'falling', 'in', 'love', 'you', 'need', 'to', 'be', 'happy', 'with', 'yourself', 'or', 'youll', 'never', 'be', 'able', 'to', 'be', 'happy', 'in', 'a', 'relationship']\n"
     ]
    }
   ],
   "source": [
    "# tokenize and look at the tokens\n",
    "data_tokens = word_tokenize(data_superfluous_blanks_removed)\n",
    "print(data_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:37.313662300Z",
     "start_time": "2023-10-13T07:48:37.176766100Z"
    }
   },
   "id": "f99ee15f6b298d43"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import stop_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.320648Z",
     "start_time": "2023-10-13T07:48:37.217451500Z"
    }
   },
   "id": "3fe99f391693cedf"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "stopwords = stop_words.STOP_WORDS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.351868500Z",
     "start_time": "2023-10-13T07:48:50.320648Z"
    }
   },
   "id": "d19ffff9fca40b0d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# remove stopwords and look at the new token list\n",
    "data_stopwords_removed = []\n",
    "for token in data_tokens:\n",
    "    if token not in stopwords:\n",
    "        data_stopwords_removed.append(token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.405257600Z",
     "start_time": "2023-10-13T07:48:50.336242100Z"
    }
   },
   "id": "17a1e65e0541bf56"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'girl', 'read', 'fairy', 'tales', 'fairy', 'tales', 'meet', 'prince', 'charming', 'hes', 'wanted', 'fairy', 'tales', 'bad', 'guy', 'easy', 'spot', 'bad', 'guy', 'wearing', 'black', 'cape', 'know', 'grow', 'realize', 'prince', 'charming', 'easy', 'find', 'thought', 'realize', 'bad', 'guy', 'wearing', 'black', 'cape', 'hes', 'easy', 'spot', 'hes', 'funny', 'makes', 'laugh', 'perfect', 'hair', 'suffer', 'girlnextdooritis', 'guy', 'friends', 'thats', '’', 've', 'learned', '’', 't', 'predict', 'love', 'plan', 'like', 'obsessed', 'organization', 'planning', 'love', 'idea', 'love', 'exception', 'love', 'wild', 'card', 'high', 'school', 'think', 'like', 'sooooo', 'cool', 'guy', 'awesome', 'car', 'matters', 'days', 'look', 'character', 'honesty', 'trust', 'write', 'songs', 'adventures', 'misadventures', 'concern', 'love', 'love', 'tricky', 'business', 'wasnt', 'wouldnt', 'enthralled', 'lately', 'ive', 'come', 'wonderful', 'realization', 'makes', 'fascinated', 'idea', 'im', 'comes', 'love', 'theres', 'pattern', 'happens', 'course', 'cant', 'plan', 'cant', 'predict', 'itll', 'end', 'love', 'unpredictable', 'frustrating', 'tragic', 'beautiful', 'theres', 'way', 'feel', 'like', 'im', 'expert', 'worth', 'writing', 'songs', 'ive', 'experienced', 'life', 'love', 'like', 'love', 'sparkles', 'having', 'word', 'thats', 'real', 'love', 'write', 'lot', 'songs', 'love', 'think', '’', 's', 'love', 'like', 'huge', 'complicated', 'thing', 'like', 'people', 'figured', 'people', 'right', 'think', 'rest', 'walk', 'daydreaming', 'like', 'find', 'great', 'love', 'sudden', 'complicated', 'simple', 'wrong', 'sudden', 'right', 'person', 'feel', 'fearless', 'leave', 'stay', 'stay', 'youre', 'horrible', 'im', 'going', 'write', 'song', 'wont', 'like', 'thats', 'operate', 'human', 'beings', 'cant', 'reply', 'head', 'sleep', 'love', 'sparkles', 'grocery', 'shopping', 'old', 'cats', 'nice', 'half', 'time', 'love', 'writing', 'journal', 'wearing', 'dresses', 'time', 'staring', 'chandeliers', 'love', 'people', 'getting', 'look', 'eye', 'tell', 'favorite', 'night', 'love', 'fall', 'love', 'falls', 'breaks', 'relationship', 'person', 'support', 'lift', 'im', 'opposed', 'falling', 'love', 'need', 'happy', 'youll', 'able', 'happy', 'relationship']\n"
     ]
    }
   ],
   "source": [
    "print(data_stopwords_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.405257600Z",
     "start_time": "2023-10-13T07:48:50.351868500Z"
    }
   },
   "id": "47f1a2ec6dad5497"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# making sure we removed stuff\n",
    "print(len(data_tokens) > len(data_stopwords_removed))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.405257600Z",
     "start_time": "2023-10-13T07:48:50.367497800Z"
    }
   },
   "id": "25dc8b11accba3d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.3: Stemming and lemmatization\n",
    "1. Perform stemming.\n",
    "2. Perform lemmatization.\n",
    "Hints:\n",
    "• The Python package ’nltk.stem’ includes the function ’PorterStemmer’.\n",
    "• The Python package ’nltk.stem’ includes the function ’WordNetLemmatizer’."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6abd7c76cfb0e4e"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jassi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(nltk.stem.PorterStemmer)\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.583763100Z",
     "start_time": "2023-10-13T07:48:50.383118500Z"
    }
   },
   "id": "32c3a5146c92f739"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['littl', 'girl', 'read', 'fairi', 'tale', 'fairi', 'tale', 'meet', 'princ', 'charm', 'he', 'want', 'fairi', 'tale', 'bad', 'guy', 'easi', 'spot', 'bad', 'guy', 'wear', 'black', 'cape', 'know', 'grow', 'realiz', 'princ', 'charm', 'easi', 'find', 'thought', 'realiz', 'bad', 'guy', 'wear', 'black', 'cape', 'he', 'easi', 'spot', 'he', 'funni', 'make', 'laugh', 'perfect', 'hair', 'suffer', 'girlnextdoor', 'guy', 'friend', 'that', '’', 've', 'learn', '’', 't', 'predict', 'love', 'plan', 'like', 'obsess', 'organ', 'plan', 'love', 'idea', 'love', 'except', 'love', 'wild', 'card', 'high', 'school', 'think', 'like', 'sooooo', 'cool', 'guy', 'awesom', 'car', 'matter', 'day', 'look', 'charact', 'honesti', 'trust', 'write', 'song', 'adventur', 'misadventur', 'concern', 'love', 'love', 'tricki', 'busi', 'wasnt', 'wouldnt', 'enthral', 'late', 'ive', 'come', 'wonder', 'realiz', 'make', 'fascin', 'idea', 'im', 'come', 'love', 'there', 'pattern', 'happen', 'cours', 'cant', 'plan', 'cant', 'predict', 'itll', 'end', 'love', 'unpredict', 'frustrat', 'tragic', 'beauti', 'there', 'way', 'feel', 'like', 'im', 'expert', 'worth', 'write', 'song', 'ive', 'experienc', 'life', 'love', 'like', 'love', 'sparkl', 'have', 'word', 'that', 'real', 'love', 'write', 'lot', 'song', 'love', 'think', '’', 's', 'love', 'like', 'huge', 'complic', 'thing', 'like', 'peopl', 'figur', 'peopl', 'right', 'think', 'rest', 'walk', 'daydream', 'like', 'find', 'great', 'love', 'sudden', 'complic', 'simpl', 'wrong', 'sudden', 'right', 'person', 'feel', 'fearless', 'leav', 'stay', 'stay', 'your', 'horribl', 'im', 'go', 'write', 'song', 'wont', 'like', 'that', 'oper', 'human', 'be', 'cant', 'repli', 'head', 'sleep', 'love', 'sparkl', 'groceri', 'shop', 'old', 'cat', 'nice', 'half', 'time', 'love', 'write', 'journal', 'wear', 'dress', 'time', 'stare', 'chandeli', 'love', 'peopl', 'get', 'look', 'eye', 'tell', 'favorit', 'night', 'love', 'fall', 'love', 'fall', 'break', 'relationship', 'person', 'support', 'lift', 'im', 'oppos', 'fall', 'love', 'need', 'happi', 'youll', 'abl', 'happi', 'relationship']\n"
     ]
    }
   ],
   "source": [
    "# stem tokens and look at the resulting stems\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "data_stemmed = [stemmer.stem(stem) for stem in data_stopwords_removed]\n",
    "print(data_stemmed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:50.583763100Z",
     "start_time": "2023-10-13T07:48:50.467775700Z"
    }
   },
   "id": "62dd8e60b3363cbe"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'girl', 'read', 'fairy', 'tale', 'fairy', 'tale', 'meet', 'prince', 'charming', 'he', 'wanted', 'fairy', 'tale', 'bad', 'guy', 'easy', 'spot', 'bad', 'guy', 'wearing', 'black', 'cape', 'know', 'grow', 'realize', 'prince', 'charming', 'easy', 'find', 'thought', 'realize', 'bad', 'guy', 'wearing', 'black', 'cape', 'he', 'easy', 'spot', 'he', 'funny', 'make', 'laugh', 'perfect', 'hair', 'suffer', 'girlnextdooritis', 'guy', 'friend', 'thats', '’', 've', 'learned', '’', 't', 'predict', 'love', 'plan', 'like', 'obsessed', 'organization', 'planning', 'love', 'idea', 'love', 'exception', 'love', 'wild', 'card', 'high', 'school', 'think', 'like', 'sooooo', 'cool', 'guy', 'awesome', 'car', 'matter', 'day', 'look', 'character', 'honesty', 'trust', 'write', 'song', 'adventure', 'misadventure', 'concern', 'love', 'love', 'tricky', 'business', 'wasnt', 'wouldnt', 'enthralled', 'lately', 'ive', 'come', 'wonderful', 'realization', 'make', 'fascinated', 'idea', 'im', 'come', 'love', 'there', 'pattern', 'happens', 'course', 'cant', 'plan', 'cant', 'predict', 'itll', 'end', 'love', 'unpredictable', 'frustrating', 'tragic', 'beautiful', 'there', 'way', 'feel', 'like', 'im', 'expert', 'worth', 'writing', 'song', 'ive', 'experienced', 'life', 'love', 'like', 'love', 'sparkle', 'having', 'word', 'thats', 'real', 'love', 'write', 'lot', 'song', 'love', 'think', '’', 's', 'love', 'like', 'huge', 'complicated', 'thing', 'like', 'people', 'figured', 'people', 'right', 'think', 'rest', 'walk', 'daydreaming', 'like', 'find', 'great', 'love', 'sudden', 'complicated', 'simple', 'wrong', 'sudden', 'right', 'person', 'feel', 'fearless', 'leave', 'stay', 'stay', 'youre', 'horrible', 'im', 'going', 'write', 'song', 'wont', 'like', 'thats', 'operate', 'human', 'being', 'cant', 'reply', 'head', 'sleep', 'love', 'sparkle', 'grocery', 'shopping', 'old', 'cat', 'nice', 'half', 'time', 'love', 'writing', 'journal', 'wearing', 'dress', 'time', 'staring', 'chandelier', 'love', 'people', 'getting', 'look', 'eye', 'tell', 'favorite', 'night', 'love', 'fall', 'love', 'fall', 'break', 'relationship', 'person', 'support', 'lift', 'im', 'opposed', 'falling', 'love', 'need', 'happy', 'youll', 'able', 'happy', 'relationship']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize tokens and look at the resulting lemmas\n",
    "lemmtizer = nltk.stem.WordNetLemmatizer()\n",
    "data_lemmatized = [lemmtizer.lemmatize(lemma) for lemma in data_stopwords_removed]\n",
    "print(data_lemmatized)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.028741700Z",
     "start_time": "2023-10-13T07:48:50.505587300Z"
    }
   },
   "id": "cbaf2511bbb35427"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.4: Vocabulary\n",
    "Obtain a vocabulary of words (a ’dictionary’ objects that contains all the unique words in the quotes.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "382fc5f0df01e4bc"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.028741700Z",
     "start_time": "2023-10-13T07:48:52.013109400Z"
    }
   },
   "id": "337f5b34948fcfa3"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able' 'adventure' 'awesome' 'bad' 'beautiful' 'being' 'black' 'break'\n",
      " 'business' 'cant' 'cape' 'car' 'card' 'cat' 'chandelier' 'character'\n",
      " 'charming' 'come' 'complicated' 'concern' 'cool' 'course' 'day'\n",
      " 'daydreaming' 'dress' 'easy' 'end' 'enthralled' 'exception' 'experienced'\n",
      " 'expert' 'eye' 'fairy' 'fall' 'falling' 'fascinated' 'favorite'\n",
      " 'fearless' 'feel' 'figured' 'find' 'friend' 'frustrating' 'funny'\n",
      " 'getting' 'girl' 'girlnextdooritis' 'going' 'great' 'grocery' 'grow'\n",
      " 'guy' 'hair' 'half' 'happens' 'happy' 'having' 'he' 'head' 'high'\n",
      " 'honesty' 'horrible' 'huge' 'human' 'idea' 'im' 'itll' 'ive' 'journal'\n",
      " 'know' 'lately' 'laugh' 'learned' 'leave' 'life' 'lift' 'like' 'little'\n",
      " 'look' 'lot' 'love' 'make' 'matter' 'meet' 'misadventure' 'need' 'nice'\n",
      " 'night' 'obsessed' 'old' 'operate' 'opposed' 'organization' 'pattern'\n",
      " 'people' 'perfect' 'person' 'plan' 'planning' 'predict' 'prince' 'read'\n",
      " 'real' 'realization' 'realize' 'relationship' 'reply' 'rest' 'right' 's'\n",
      " 'school' 'shopping' 'simple' 'sleep' 'song' 'sooooo' 'sparkle' 'spot'\n",
      " 'staring' 'stay' 'sudden' 'suffer' 'support' 't' 'tale' 'tell' 'thats'\n",
      " 'there' 'thing' 'think' 'thought' 'time' 'tragic' 'tricky' 'trust'\n",
      " 'unpredictable' 've' 'walk' 'wanted' 'wasnt' 'way' 'wearing' 'wild'\n",
      " 'wonderful' 'wont' 'word' 'worth' 'wouldnt' 'write' 'writing' 'wrong'\n",
      " 'youll' 'youre' '’']\n"
     ]
    }
   ],
   "source": [
    "# not sure at this point if this should be done with the original tokens, including stopwords, excluding stopwords, or the lemmas.\n",
    "# but since we are going to use it for one-hot-encoding, it seems sensible to make the dictionary as small as possible so that the\n",
    "# one hot encoded vectors don't get huge. So imma do this with lemmas, excluding stopwords\n",
    "data_dictionary = np.unique(data_lemmatized)\n",
    "print(data_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.166851600Z",
     "start_time": "2023-10-13T07:48:52.028741700Z"
    }
   },
   "id": "543ad9fc6c74532f"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# and checking again, if the extras were removed\n",
    "print(len(data_dictionary) < len(data_lemmatized))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.198083500Z",
     "start_time": "2023-10-13T07:48:52.050891800Z"
    }
   },
   "id": "539005997193b469"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.5: One-hot encoding\n",
    "Perform one-hot encoding with respect to the vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbf7876373cbd225"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'able': 0, 'adventure': 1, 'awesome': 2, 'bad': 3, 'beautiful': 4, 'being': 5, 'black': 6, 'break': 7, 'business': 8, 'cant': 9, 'cape': 10, 'car': 11, 'card': 12, 'cat': 13, 'chandelier': 14, 'character': 15, 'charming': 16, 'come': 17, 'complicated': 18, 'concern': 19, 'cool': 20, 'course': 21, 'day': 22, 'daydreaming': 23, 'dress': 24, 'easy': 25, 'end': 26, 'enthralled': 27, 'exception': 28, 'experienced': 29, 'expert': 30, 'eye': 31, 'fairy': 32, 'fall': 33, 'falling': 34, 'fascinated': 35, 'favorite': 36, 'fearless': 37, 'feel': 38, 'figured': 39, 'find': 40, 'friend': 41, 'frustrating': 42, 'funny': 43, 'getting': 44, 'girl': 45, 'girlnextdooritis': 46, 'going': 47, 'great': 48, 'grocery': 49, 'grow': 50, 'guy': 51, 'hair': 52, 'half': 53, 'happens': 54, 'happy': 55, 'having': 56, 'he': 57, 'head': 58, 'high': 59, 'honesty': 60, 'horrible': 61, 'huge': 62, 'human': 63, 'idea': 64, 'im': 65, 'itll': 66, 'ive': 67, 'journal': 68, 'know': 69, 'lately': 70, 'laugh': 71, 'learned': 72, 'leave': 73, 'life': 74, 'lift': 75, 'like': 76, 'little': 77, 'look': 78, 'lot': 79, 'love': 80, 'make': 81, 'matter': 82, 'meet': 83, 'misadventure': 84, 'need': 85, 'nice': 86, 'night': 87, 'obsessed': 88, 'old': 89, 'operate': 90, 'opposed': 91, 'organization': 92, 'pattern': 93, 'people': 94, 'perfect': 95, 'person': 96, 'plan': 97, 'planning': 98, 'predict': 99, 'prince': 100, 'read': 101, 'real': 102, 'realization': 103, 'realize': 104, 'relationship': 105, 'reply': 106, 'rest': 107, 'right': 108, 's': 109, 'school': 110, 'shopping': 111, 'simple': 112, 'sleep': 113, 'song': 114, 'sooooo': 115, 'sparkle': 116, 'spot': 117, 'staring': 118, 'stay': 119, 'sudden': 120, 'suffer': 121, 'support': 122, 't': 123, 'tale': 124, 'tell': 125, 'thats': 126, 'there': 127, 'thing': 128, 'think': 129, 'thought': 130, 'time': 131, 'tragic': 132, 'tricky': 133, 'trust': 134, 'unpredictable': 135, 've': 136, 'walk': 137, 'wanted': 138, 'wasnt': 139, 'way': 140, 'wearing': 141, 'wild': 142, 'wonderful': 143, 'wont': 144, 'word': 145, 'worth': 146, 'wouldnt': 147, 'write': 148, 'writing': 149, 'wrong': 150, 'youll': 151, 'youre': 152, '’': 153}\n"
     ]
    }
   ],
   "source": [
    "# making a dictionary where each entry looks like this:\n",
    "# key:value\n",
    "# token:index\n",
    "# so that later we can easily get the index if we know the token\n",
    "token_to_index = {}\n",
    "for i, token in enumerate(data_dictionary):\n",
    "    token_to_index[token] = i\n",
    "\n",
    "print(token_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.198083500Z",
     "start_time": "2023-10-13T07:48:52.066535400Z"
    }
   },
   "id": "780cc73b4d61b95f"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.])]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each token in the dataset, make an array that has the size of our total dictionary\n",
    "# but should i do this quotewise? Like, make a list of lists (quotes) of lists (one-hot-encoded tokens)? because right now it's a list (all quotes in one) of lists (one-hot-encoded vectors)\n",
    "data_one_hot_encoded = []\n",
    "for token in data_lemmatized:\n",
    "    vector = np.zeros(len(data_dictionary))\n",
    "    vector[token_to_index[token]] = 1\n",
    "    data_one_hot_encoded.append(vector)\n",
    "# there is a lot of stuff in there so i'm just gonna show the first few entries\n",
    "data_one_hot_encoded[0:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.198083500Z",
     "start_time": "2023-10-13T07:48:52.082160400Z"
    }
   },
   "id": "13804a5f7fe128eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.6: Bag-of-words\n",
    "Encode every quote in the dataset as a bag-of-words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eb9e124be2f25ae"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# ok so it says to encode every quote, which i can see makes sense. i will split them at the newlines and store them\n",
    "quotes = (data_superfluous_blanks_removed.splitlines())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.198083500Z",
     "start_time": "2023-10-13T07:48:52.106836300Z"
    }
   },
   "id": "c8113dd4eeaeb22a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# tokenize each quote\n",
    "quotes_tokens = []\n",
    "for quote in quotes:\n",
    "    quotes_tokens.append(word_tokenize(quote))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.198083500Z",
     "start_time": "2023-10-13T07:48:52.122460800Z"
    }
   },
   "id": "6ee3e6cad4f07e22"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# need to lemmatize to be able to work with the dictionary.\n",
    "quote_tokens_lemmatized = []\n",
    "for quote in quotes_tokens:\n",
    "    quote_tokens_lemmatized.append([lemmtizer.lemmatize(lemma) for lemma in quote])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.244941400Z",
     "start_time": "2023-10-13T07:48:52.138125100Z"
    }
   },
   "id": "ef0abaf2034ac04d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.]),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        1.])]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar process as with one-hot encoding\n",
    "# per quote, make an array of zeros and set the indices corresponding in the dictionary to 1 for each occuring token\n",
    "bags_of_words = []\n",
    "for quote in quote_tokens_lemmatized:\n",
    "    bag_of_words = np.zeros(len(data_dictionary))\n",
    "    for token in quote:\n",
    "        if token in data_dictionary:\n",
    "            bag_of_words[token_to_index[token]] = 1\n",
    "    bags_of_words.append(bag_of_words)\n",
    "bags_of_words[0:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.429926600Z",
     "start_time": "2023-10-13T07:48:52.166851600Z"
    }
   },
   "id": "cb5793b0d4361684"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.7: Bag-of-n-grams\n",
    "1. Encode every quote as a bag-of-bigrams.\n",
    "2. Encode every quote as a bag-of-trigrams.\n",
    "Hint: The Python package ’sklearn.feature_extraction.text’ includes the function ’CountVectorizer’."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4d5ca655b6a107f"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# i don't know if i should use the version with or without stopwords now, so i will follow this person's advice:\n",
    "# https://stats.stackexchange.com/a/570699\n",
    "# First calculate the n-grams, then remove n-grams containing stopwords, but only remove n-grams which begin and/or end with a stopword.\n",
    "\n",
    "# i also use this for reference on how to make ngrams from a list of tokens:\n",
    "# https://datascience.stackexchange.com/a/48068"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.501102600Z",
     "start_time": "2023-10-13T07:48:52.198083500Z"
    }
   },
   "id": "bea77dec48d54b60"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.501102600Z",
     "start_time": "2023-10-13T07:48:52.213717500Z"
    }
   },
   "id": "e4a3779b2c05bffc"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def bigrams_wrapper(sent):\n",
    "    return list(nltk.ngrams(sent, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.501102600Z",
     "start_time": "2023-10-13T07:48:52.229324200Z"
    }
   },
   "id": "fdd37259e843c4d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "bigrams = map(bigrams_wrapper, quotes_tokens)\n",
    "quotes_bigrams = list(itertools.chain.from_iterable(bigrams))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.501102600Z",
     "start_time": "2023-10-13T07:48:52.251462900Z"
    }
   },
   "id": "29b496cfd8def2f9"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "bigrams_filtered = []\n",
    "for bigram in quotes_bigrams:\n",
    "    if bigram[0] not in stopwords and bigram[1] not in stopwords:\n",
    "        bigrams_filtered.append(bigram)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.532351Z",
     "start_time": "2023-10-13T07:48:52.267115700Z"
    }
   },
   "id": "8eb8eb9afbb1bf53"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('writing', 'songs'), ('great', 'love'), ('’', 's'), ('’', 't'), ('love', 'love'), ('youre', 'horrible'), ('human', 'beings'), ('falls', 'breaks'), ('spot', 'hes'), ('wont', 'like'), ('cant', 'predict'), ('bad', 'guy'), ('ive', 'come'), ('love', 'sparkles'), ('im', 'going'), ('fairy', 'tales'), ('write', 'songs'), ('high', 'school'), ('like', 'im'), ('sooooo', 'cool'), ('lately', 'ive'), ('huge', 'complicated'), ('prince', 'charming'), ('perfect', 'hair'), ('wonderful', 'realization'), ('black', 'cape'), ('read', 'fairy'), ('love', 'writing'), ('t', 'predict'), ('wearing', 'dresses'), ('wild', 'card'), ('little', 'girl'), ('itll', 'end'), ('ve', 'learned'), ('grocery', 'shopping'), ('’', 've'), ('like', 'sooooo'), ('tricky', 'business'), ('feel', 'like'), ('real', 'love'), ('awesome', 'car'), ('complicated', 'thing'), ('feel', 'fearless'), ('thats', 'real'), ('meet', 'prince'), ('predict', 'love'), ('old', 'cats'), ('worth', 'writing'), ('cant', 'plan'), ('concern', 'love')]\n"
     ]
    }
   ],
   "source": [
    "bigram_dictionary = list(set(bigrams_filtered))\n",
    "print(bigram_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.282739800Z"
    }
   },
   "id": "7883410dee42a2ec"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# ok this is too much work rn.\n",
    "# i'd like to continue but imma go and use the suggested library and finish this other approach if i can make some time later"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.298368400Z"
    }
   },
   "id": "66fcccbe358cd9e3"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.313994200Z"
    }
   },
   "id": "6a5890cf54951977"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 504)\n"
     ]
    }
   ],
   "source": [
    "# bigrams\n",
    "vectorizer_bi = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X_bi = vectorizer_bi.fit_transform(quotes)\n",
    "vectorizer_bi.get_feature_names_out()\n",
    "quotes_to_bigrams = X_bi.toarray()\n",
    "print(X_bi.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.329613500Z"
    }
   },
   "id": "90391b02efe1687f"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 535)\n"
     ]
    }
   ],
   "source": [
    "# trigram\n",
    "vectorizer_tri = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "X_tri = vectorizer_tri.fit_transform(quotes)\n",
    "vectorizer_tri.get_feature_names_out()\n",
    "quotes_to_trigrams = X_tri.toarray()\n",
    "print(X_tri.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.351751700Z"
    }
   },
   "id": "a1b7738468afc20a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise A.8: TF-IDF\n",
    "Encode every quote via the TF-IDF method.\n",
    "Hint: The Python package ’sklearn.feature_extraction.text’ includes the function ’TfidfVectorizer’."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a25dd1061e2042"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.367427700Z"
    }
   },
   "id": "9ccda130b3d588b1"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(quotes)\n",
    "vectorizer_tfidf.get_feature_names_out()\n",
    "print(X_tfidf.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:48:52.568266500Z",
     "start_time": "2023-10-13T07:48:52.383052600Z"
    }
   },
   "id": "97e84b5cf14f86bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
